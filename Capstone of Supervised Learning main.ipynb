{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "\n",
    "This is a project predicting Attack for Network. The dataset used for this analysis was taken from kaggle dataset. The raw network packets of the UNSW-NB15 is a comprehensive dataset for network intrusion detection systems which was created by the IXIA PerfectStorm tool in the Cyber Range Lab of the Australian Centre for Cyber Security (ACCS) for generating a hybrid of real modern normal activities and synthetic contemporary attack behaviours. It was published in 2015. This dataset has nine types of attacks, namely: Fuzzers, Analysis, Backdoors, DoS, Exploits, Generic, Reconnaissance, Shellcode and Worms. It has features with the class label. I use a partition from this dataset is configured as a training set, namely: UNSW_NB15_training-set.csv. The number of records in the dataset is 175,341 records from the different types of attack and normal.\n",
    "\n",
    "-The objective of this project is:\n",
    "\n",
    "- Exploring  data for  analysing of cyber security data\n",
    "- Perform anomaly detection using some algorithms and evaluate its learning profile and predict the anomaly detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is centered around 6 different questions:\n",
    "\n",
    "- What are the most common types of Attack?\n",
    "- What are the most common protocol,service and state for Attack?\n",
    "- What are the effect of Attack?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NOTE:\n",
    " \n",
    "The features of dataset are described in UNSW-NB15_features.csv file which says that:\n",
    "- In 'state' column: '-' means that 'Not used stste'.\n",
    "- In 'service' column: '-' means that ' Not much used service'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Liberaries and Packages:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import struct\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import jarque_bera\n",
    "from scipy.stats import normaltest\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,log_loss\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all dataset's files:\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\mebra.DESKTOP-L12LJA6\\Thinkful Works\\PythonThinkful\\capstonbotdataset\\UNSW_NB15_training-set.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>90909.0902</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1762</td>\n",
       "      <td>0</td>\n",
       "      <td>125000.0003</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1068</td>\n",
       "      <td>0</td>\n",
       "      <td>200000.0051</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "      <td>0</td>\n",
       "      <td>166666.6608</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2126</td>\n",
       "      <td>0</td>\n",
       "      <td>100000.0025</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       dur proto service state  spkts  dpkts  sbytes  dbytes  \\\n",
       "0   1  0.000011   udp       -   INT      2      0     496       0   \n",
       "1   2  0.000008   udp       -   INT      2      0    1762       0   \n",
       "2   3  0.000005   udp       -   INT      2      0    1068       0   \n",
       "3   4  0.000006   udp       -   INT      2      0     900       0   \n",
       "4   5  0.000010   udp       -   INT      2      0    2126       0   \n",
       "\n",
       "          rate  ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  \\\n",
       "0   90909.0902  ...                 1               2             0   \n",
       "1  125000.0003  ...                 1               2             0   \n",
       "2  200000.0051  ...                 1               3             0   \n",
       "3  166666.6608  ...                 1               3             0   \n",
       "4  100000.0025  ...                 1               3             0   \n",
       "\n",
       "   ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  \\\n",
       "0           0                 0           1           2                0   \n",
       "1           0                 0           1           2                0   \n",
       "2           0                 0           1           3                0   \n",
       "3           0                 0           2           3                0   \n",
       "4           0                 0           2           3                0   \n",
       "\n",
       "   attack_cat  label  \n",
       "0      Normal      0  \n",
       "1      Normal      0  \n",
       "2      Normal      0  \n",
       "3      Normal      0  \n",
       "4      Normal      0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the dataset:\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82332, 45)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the shape of dataset:\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset by droping duplicates: \n",
    "\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82332"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the length of dataset after removing duplicate:\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82332 entries, 0 to 82331\n",
      "Data columns (total 45 columns):\n",
      "id                   82332 non-null int64\n",
      "dur                  82332 non-null float64\n",
      "proto                82332 non-null object\n",
      "service              82332 non-null object\n",
      "state                82332 non-null object\n",
      "spkts                82332 non-null int64\n",
      "dpkts                82332 non-null int64\n",
      "sbytes               82332 non-null int64\n",
      "dbytes               82332 non-null int64\n",
      "rate                 82332 non-null float64\n",
      "sttl                 82332 non-null int64\n",
      "dttl                 82332 non-null int64\n",
      "sload                82332 non-null float64\n",
      "dload                82332 non-null float64\n",
      "sloss                82332 non-null int64\n",
      "dloss                82332 non-null int64\n",
      "sinpkt               82332 non-null float64\n",
      "dinpkt               82332 non-null float64\n",
      "sjit                 82332 non-null float64\n",
      "djit                 82332 non-null float64\n",
      "swin                 82332 non-null int64\n",
      "stcpb                82332 non-null int64\n",
      "dtcpb                82332 non-null int64\n",
      "dwin                 82332 non-null int64\n",
      "tcprtt               82332 non-null float64\n",
      "synack               82332 non-null float64\n",
      "ackdat               82332 non-null float64\n",
      "smean                82332 non-null int64\n",
      "dmean                82332 non-null int64\n",
      "trans_depth          82332 non-null int64\n",
      "response_body_len    82332 non-null int64\n",
      "ct_srv_src           82332 non-null int64\n",
      "ct_state_ttl         82332 non-null int64\n",
      "ct_dst_ltm           82332 non-null int64\n",
      "ct_src_dport_ltm     82332 non-null int64\n",
      "ct_dst_sport_ltm     82332 non-null int64\n",
      "ct_dst_src_ltm       82332 non-null int64\n",
      "is_ftp_login         82332 non-null int64\n",
      "ct_ftp_cmd           82332 non-null int64\n",
      "ct_flw_http_mthd     82332 non-null int64\n",
      "ct_src_ltm           82332 non-null int64\n",
      "ct_srv_dst           82332 non-null int64\n",
      "is_sm_ips_ports      82332 non-null int64\n",
      "attack_cat           82332 non-null object\n",
      "label                82332 non-null int64\n",
      "dtypes: float64(11), int64(30), object(4)\n",
      "memory usage: 28.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Look at the type of columns:\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Values in Column id are: 82332\n",
      "Number of Unique Values in Column dur are: 39888\n",
      "Number of Unique Values in Column proto are: 131\n",
      "Number of Unique Values in Column service are: 13\n",
      "Number of Unique Values in Column state are: 7\n",
      "Number of Unique Values in Column spkts are: 420\n",
      "Number of Unique Values in Column dpkts are: 436\n",
      "Number of Unique Values in Column sbytes are: 4489\n",
      "Number of Unique Values in Column dbytes are: 4034\n",
      "Number of Unique Values in Column rate are: 40616\n",
      "Number of Unique Values in Column sttl are: 11\n",
      "Number of Unique Values in Column dttl are: 8\n",
      "Number of Unique Values in Column sload are: 42873\n",
      "Number of Unique Values in Column dload are: 40614\n",
      "Number of Unique Values in Column sloss are: 253\n",
      "Number of Unique Values in Column dloss are: 311\n",
      "Number of Unique Values in Column sinpkt are: 39970\n",
      "Number of Unique Values in Column dinpkt are: 37617\n",
      "Number of Unique Values in Column sjit are: 39944\n",
      "Number of Unique Values in Column djit are: 38381\n",
      "Number of Unique Values in Column swin are: 11\n",
      "Number of Unique Values in Column stcpb are: 39219\n",
      "Number of Unique Values in Column dtcpb are: 39108\n",
      "Number of Unique Values in Column dwin are: 14\n",
      "Number of Unique Values in Column tcprtt are: 26130\n",
      "Number of Unique Values in Column synack are: 24934\n",
      "Number of Unique Values in Column ackdat are: 24020\n",
      "Number of Unique Values in Column smean are: 1282\n",
      "Number of Unique Values in Column dmean are: 1222\n",
      "Number of Unique Values in Column trans_depth are: 8\n",
      "Number of Unique Values in Column response_body_len are: 1190\n",
      "Number of Unique Values in Column ct_srv_src are: 57\n",
      "Number of Unique Values in Column ct_state_ttl are: 7\n",
      "Number of Unique Values in Column ct_dst_ltm are: 50\n",
      "Number of Unique Values in Column ct_src_dport_ltm are: 50\n",
      "Number of Unique Values in Column ct_dst_sport_ltm are: 33\n",
      "Number of Unique Values in Column ct_dst_src_ltm are: 57\n",
      "Number of Unique Values in Column is_ftp_login are: 3\n",
      "Number of Unique Values in Column ct_ftp_cmd are: 3\n",
      "Number of Unique Values in Column ct_flw_http_mthd are: 8\n",
      "Number of Unique Values in Column ct_src_ltm are: 50\n",
      "Number of Unique Values in Column ct_srv_dst are: 57\n",
      "Number of Unique Values in Column is_sm_ips_ports are: 2\n",
      "Number of Unique Values in Column attack_cat are: 10\n",
      "Number of Unique Values in Column label are: 2\n"
     ]
    }
   ],
   "source": [
    "# Findout number of unique value in each column:\n",
    "\n",
    "for col in df.columns:\n",
    "    print(\"Number of Unique Values in Column {} are: {}\".format(col, df[col].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Findout percentage of missing values in each columns:\n",
    "\n",
    "null_count = round(df.isnull().sum()*100/df.isnull().count(),2)\n",
    "null_count[null_count>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>attack_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  proto service state attack_cat\n",
       "0   udp       -   INT     Normal\n",
       "1   udp       -   INT     Normal\n",
       "2   udp       -   INT     Normal\n",
       "3   udp       -   INT     Normal\n",
       "4   udp       -   INT     Normal"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Findout object columns:\n",
    "\n",
    "object_columns = df.select_dtypes('object')\n",
    "object_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in column proto are: 131, ['udp' 'arp' 'tcp' 'igmp' 'ospf' 'sctp' 'gre' 'ggp' 'ip' 'ipnip' 'st2'\n",
      " 'argus' 'chaos' 'egp' 'emcon' 'nvp' 'pup' 'xnet' 'mux' 'dcn' 'hmp' 'prm'\n",
      " 'trunk-1' 'trunk-2' 'xns-idp' 'leaf-1' 'leaf-2' 'irtp' 'rdp' 'netblt'\n",
      " 'mfe-nsp' 'merit-inp' '3pc' 'idpr' 'ddp' 'idpr-cmtp' 'tp++' 'ipv6' 'sdrp'\n",
      " 'ipv6-frag' 'ipv6-route' 'idrp' 'mhrp' 'i-nlsp' 'rvd' 'mobile' 'narp'\n",
      " 'skip' 'tlsp' 'ipv6-no' 'any' 'ipv6-opts' 'cftp' 'sat-expak' 'ippc'\n",
      " 'kryptolan' 'sat-mon' 'cpnx' 'wsn' 'pvp' 'br-sat-mon' 'sun-nd' 'wb-mon'\n",
      " 'vmtp' 'ttp' 'vines' 'nsfnet-igp' 'dgp' 'eigrp' 'tcf' 'sprite-rpc' 'larp'\n",
      " 'mtp' 'ax.25' 'ipip' 'aes-sp3-d' 'micp' 'encap' 'pri-enc' 'gmtp' 'ifmp'\n",
      " 'pnni' 'qnx' 'scps' 'cbt' 'bbn-rcc' 'igp' 'bna' 'swipe' 'visa' 'ipcv'\n",
      " 'cphb' 'iso-tp4' 'wb-expak' 'sep' 'secure-vmtp' 'xtp' 'il' 'rsvp' 'unas'\n",
      " 'fc' 'iso-ip' 'etherip' 'pim' 'aris' 'a/n' 'ipcomp' 'snp' 'compaq-peer'\n",
      " 'ipx-n-ip' 'pgm' 'vrrp' 'l2tp' 'zero' 'ddx' 'iatp' 'stp' 'srp' 'uti' 'sm'\n",
      " 'smp' 'isis' 'ptp' 'fire' 'crtp' 'crudp' 'sccopmce' 'iplt' 'pipe' 'sps'\n",
      " 'ib']\n",
      "Unique values in column service are: 13, ['-' 'http' 'ftp' 'ftp-data' 'smtp' 'pop3' 'dns' 'snmp' 'ssl' 'dhcp' 'irc'\n",
      " 'radius' 'ssh']\n",
      "Unique values in column state are: 7, ['INT' 'FIN' 'REQ' 'ACC' 'CON' 'RST' 'CLO']\n",
      "Unique values in column attack_cat are: 10, ['Normal' 'Reconnaissance' 'Backdoor' 'DoS' 'Exploits' 'Analysis'\n",
      " 'Fuzzers' 'Worms' 'Shellcode' 'Generic']\n"
     ]
    }
   ],
   "source": [
    "# Findout unique values in each object columns:\n",
    "\n",
    "for col in object_columns:\n",
    "    print(\"Unique values in column {} are: {}, {}\".format(col, df[col].nunique(), df[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reolace the '-' to null:\n",
    "\n",
    "df['service'] = df['service'].replace('-', 'else')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['else', 'http', 'ftp', 'ftp-data', 'smtp', 'pop3', 'dns', 'snmp',\n",
       "       'ssl', 'dhcp', 'irc', 'radius', 'ssh'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the unique value of service column:\n",
    "df[\"service\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['INT', 'FIN', 'REQ', 'ACC', 'CON', 'RST', 'CLO'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the unique value of state column:\n",
    "df[\"state\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>attack_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>82332</td>\n",
       "      <td>82332</td>\n",
       "      <td>82332</td>\n",
       "      <td>82332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>131</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>tcp</td>\n",
       "      <td>else</td>\n",
       "      <td>FIN</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>43095</td>\n",
       "      <td>47153</td>\n",
       "      <td>39339</td>\n",
       "      <td>37000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        proto service  state attack_cat\n",
       "count   82332   82332  82332      82332\n",
       "unique    131      13      7         10\n",
       "top       tcp    else    FIN     Normal\n",
       "freq    43095   47153  39339      37000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive statistics for object variables:\n",
    "\n",
    "df.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dur</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sload</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>8.233200e+04</td>\n",
       "      <td>8.233200e+04</td>\n",
       "      <td>8.233200e+04</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>8.233200e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "      <td>82332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41166.500000</td>\n",
       "      <td>1.006756</td>\n",
       "      <td>18.666472</td>\n",
       "      <td>17.545936</td>\n",
       "      <td>7.993908e+03</td>\n",
       "      <td>1.323379e+04</td>\n",
       "      <td>8.241089e+04</td>\n",
       "      <td>180.967667</td>\n",
       "      <td>95.713003</td>\n",
       "      <td>6.454902e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>4.928898</td>\n",
       "      <td>3.663011</td>\n",
       "      <td>7.456360</td>\n",
       "      <td>0.008284</td>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.129743</td>\n",
       "      <td>6.468360</td>\n",
       "      <td>9.164262</td>\n",
       "      <td>0.011126</td>\n",
       "      <td>0.550600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>23767.345519</td>\n",
       "      <td>4.710444</td>\n",
       "      <td>133.916353</td>\n",
       "      <td>115.574086</td>\n",
       "      <td>1.716423e+05</td>\n",
       "      <td>1.514715e+05</td>\n",
       "      <td>1.486204e+05</td>\n",
       "      <td>101.513358</td>\n",
       "      <td>116.667722</td>\n",
       "      <td>1.798618e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>8.389545</td>\n",
       "      <td>5.915386</td>\n",
       "      <td>11.415191</td>\n",
       "      <td>0.091171</td>\n",
       "      <td>0.092485</td>\n",
       "      <td>0.638683</td>\n",
       "      <td>8.543927</td>\n",
       "      <td>11.121413</td>\n",
       "      <td>0.104891</td>\n",
       "      <td>0.497436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20583.750000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.140000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.860611e+01</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.120247e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>41166.500000</td>\n",
       "      <td>0.014138</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.340000e+02</td>\n",
       "      <td>1.780000e+02</td>\n",
       "      <td>2.650177e+03</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.770032e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61749.250000</td>\n",
       "      <td>0.719360</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.280000e+03</td>\n",
       "      <td>9.560000e+02</td>\n",
       "      <td>1.111111e+05</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>6.514286e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>82332.000000</td>\n",
       "      <td>59.999989</td>\n",
       "      <td>10646.000000</td>\n",
       "      <td>11018.000000</td>\n",
       "      <td>1.435577e+07</td>\n",
       "      <td>1.465753e+07</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>5.268000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id           dur         spkts         dpkts        sbytes  \\\n",
       "count  82332.000000  82332.000000  82332.000000  82332.000000  8.233200e+04   \n",
       "mean   41166.500000      1.006756     18.666472     17.545936  7.993908e+03   \n",
       "std    23767.345519      4.710444    133.916353    115.574086  1.716423e+05   \n",
       "min        1.000000      0.000000      1.000000      0.000000  2.400000e+01   \n",
       "25%    20583.750000      0.000008      2.000000      0.000000  1.140000e+02   \n",
       "50%    41166.500000      0.014138      6.000000      2.000000  5.340000e+02   \n",
       "75%    61749.250000      0.719360     12.000000     10.000000  1.280000e+03   \n",
       "max    82332.000000     59.999989  10646.000000  11018.000000  1.435577e+07   \n",
       "\n",
       "             dbytes          rate          sttl          dttl         sload  \\\n",
       "count  8.233200e+04  8.233200e+04  82332.000000  82332.000000  8.233200e+04   \n",
       "mean   1.323379e+04  8.241089e+04    180.967667     95.713003  6.454902e+07   \n",
       "std    1.514715e+05  1.486204e+05    101.513358    116.667722  1.798618e+08   \n",
       "min    0.000000e+00  0.000000e+00      0.000000      0.000000  0.000000e+00   \n",
       "25%    0.000000e+00  2.860611e+01     62.000000      0.000000  1.120247e+04   \n",
       "50%    1.780000e+02  2.650177e+03    254.000000     29.000000  5.770032e+05   \n",
       "75%    9.560000e+02  1.111111e+05    254.000000    252.000000  6.514286e+07   \n",
       "max    1.465753e+07  1.000000e+06    255.000000    253.000000  5.268000e+09   \n",
       "\n",
       "       ...  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  \\\n",
       "count  ...      82332.000000      82332.000000    82332.000000  82332.000000   \n",
       "mean   ...          4.928898          3.663011        7.456360      0.008284   \n",
       "std    ...          8.389545          5.915386       11.415191      0.091171   \n",
       "min    ...          1.000000          1.000000        1.000000      0.000000   \n",
       "25%    ...          1.000000          1.000000        1.000000      0.000000   \n",
       "50%    ...          1.000000          1.000000        3.000000      0.000000   \n",
       "75%    ...          4.000000          3.000000        6.000000      0.000000   \n",
       "max    ...         59.000000         38.000000       63.000000      2.000000   \n",
       "\n",
       "         ct_ftp_cmd  ct_flw_http_mthd    ct_src_ltm    ct_srv_dst  \\\n",
       "count  82332.000000      82332.000000  82332.000000  82332.000000   \n",
       "mean       0.008381          0.129743      6.468360      9.164262   \n",
       "std        0.092485          0.638683      8.543927     11.121413   \n",
       "min        0.000000          0.000000      1.000000      1.000000   \n",
       "25%        0.000000          0.000000      1.000000      2.000000   \n",
       "50%        0.000000          0.000000      3.000000      5.000000   \n",
       "75%        0.000000          0.000000      7.000000     11.000000   \n",
       "max        2.000000         16.000000     60.000000     62.000000   \n",
       "\n",
       "       is_sm_ips_ports         label  \n",
       "count     82332.000000  82332.000000  \n",
       "mean          0.011126      0.550600  \n",
       "std           0.104891      0.497436  \n",
       "min           0.000000      0.000000  \n",
       "25%           0.000000      0.000000  \n",
       "50%           0.000000      1.000000  \n",
       "75%           0.000000      1.000000  \n",
       "max           1.000000      1.000000  \n",
       "\n",
       "[8 rows x 41 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get univariate statistics for numeric columns:\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of dataset :\n",
    "df_main = df.copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the correlation between columns:\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.heatmap(df.corr(), annot=True, linewidths=.5, fmt= '.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at the correlation between Attack and other columns:\n",
    "np.abs(df.corr())[['label']].sort_values(by='label', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Findout numeric columns:\n",
    "\n",
    "numeric_columns = df.select_dtypes(exclude=['object']).columns\n",
    "print('Number of numeric columns is {}'.format(len(numeric_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Univariate visualization of continuous variables by using hist:\n",
    "\n",
    "plt.figure(figsize=(25,50))\n",
    "for i,col in enumerate(numeric_columns.drop('id')):\n",
    "    plt.subplot(11, 4, i+1)\n",
    "    plt.boxplot(df[col])\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=90)\n",
    "plt.subplots_adjust(hspace = 0.8, top = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Findout non_numeric columns:\n",
    "\n",
    "nonnumeric_columns = df.select_dtypes(['object']).columns\n",
    "print('Number of non_numeric columns is {}'.format(len(nonnumeric_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Univariate visualization of categorical variables by barplot:\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "for i,col in enumerate(nonnumeric_columns):\n",
    "    plt.subplot(4, 1, i+1)\n",
    "    sns.countplot(df[col])\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "plt.subplots_adjust(hspace = 0.8, top = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most Common:** \n",
    "\n",
    "- TCP and UDP are most common protocols were used in the dataset, also TCP was used more than UDP. \n",
    "\n",
    "- DNS and other services are most common services were used, also other services used more than DNS.\n",
    "\n",
    "- INT and FIN are most common states were used and INT was used more.\n",
    "\n",
    "- The number of Normal records is greater than Attack records. \n",
    "\n",
    "- The most common Attack was occured in the dataset is Genericand then Exploits. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis of continuous variables by scatter plot:\n",
    "\n",
    "plt.figure(figsize=(25,50))\n",
    "for i,col in enumerate(numeric_columns.drop('id')):\n",
    "    plt.subplot(11, 4, i+1)\n",
    "    sns.boxplot(x=df[col], hue=df['label'], data=df)\n",
    "    plt.title(f'Distribution of label by {col}')\n",
    "    plt.xticks(rotation=90)\n",
    "plt.subplots_adjust(hspace = 0.8, top = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keep top 6 protocols:\n",
    "df['proto'].value_counts()[:6].sum()/df.shape[0]*100\n",
    "proto_other_lst = list(df['proto'].value_counts()[7:].index)\n",
    "df['proto'] = df['proto'].apply(lambda x: x if x not in proto_other_lst else 'other')\n",
    "df['proto'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bivariate analysis of a continuous-categorical pair:\n",
    "\n",
    "plt.figure(figsize=(15,30))\n",
    "for i,col in enumerate(nonnumeric_columns):\n",
    "    plt.subplot(4,1,i+1)\n",
    "    sns.barplot(df[col], df['rate'])\n",
    "    plt.title(f'Distribution of {col} by rate')\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "plt.subplots_adjust(hspace = 0.8, top = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis of a continuous-categorical pair:\n",
    "\n",
    "plt.figure(figsize=(25,40))\n",
    "for i,col in enumerate(nonnumeric_columns):\n",
    "    plt.subplot(4,1,i+1)\n",
    "    df.groupby(col).label.value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of {col} and label')\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "plt.subplots_adjust(hspace = 0.8, top = 0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most common protocol, service, state:**\n",
    "- In TCP and UDP protocol, number of Normal records is  greater thann Attack records.\n",
    "- Most common protocol for Attack is UDP.\n",
    "- Most common servic for Attack is DNS.\n",
    "- Most common state for Attack is INT.\n",
    "- The number of Normal records in other services is greater than Attack records. But in DNS the numbr of Attack records is greater than normal.\n",
    "- The number of Attack records in INT is greater then Normal. But in FIN the number of Normal records is greater than Attack records. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis of a continuous-categorical pair:\n",
    "plt.figure(figsize=(30,50))\n",
    "for i,col in enumerate(numeric_columns.drop('id')):\n",
    "    plt.subplot(12,4,i+1)\n",
    "    sns.barplot(df['attack_cat'], df[col])\n",
    "    plt.title(f'Distribution of Attac_cat by {col}')\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "plt.subplots_adjust(hspace = 1.5, top = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Attack:\n",
    "\n",
    "- Dos and Fuzzers Attacks have most total duration of records.\n",
    "- Exploits Attack has maximum number of packets (bytes) from source to destination.\n",
    "- Worms Attack has maximum number of packets (bytes) from destination to sourc.\n",
    "- Generic Attack has most total transaction packets per second.\n",
    "- Normal records have lowest time to live value from Source to destination.\n",
    "- Worms Aattack has longest time to live value from destination to source.\n",
    "- Normal records have minimum source bits per second, but for destination have maximum.\n",
    "- Exploits Attack has maximum number of source packets retransmitted or dropped. \n",
    "- Worms Attack has maximum number of destination packets retransmitted or dropped. \n",
    "- Fuzzers and Exploits Attacks have maximum mean of the flow packet size transmitted by the src, for destination Worms Attack is maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using Piechart to see distribution of source and destination in each attack_cat:\n",
    "\n",
    "cols=['spkts','sload','sbytes','dpkts','dload','dbytes','sttl','dttl','sloss','dloss','smean','dmean','swin', 'dwin','dur','rate']\n",
    "df_attack = df.groupby('attack_cat')\n",
    "plt.figure(figsize=(40,50))\n",
    "\n",
    "for i,col in enumerate(cols):\n",
    "    plt.subplot(7,3,i+1)\n",
    "    df_attack[col].sum().plot(kind='pie',  title=(f'Ditribution of {col} in each attack_cat'), autopct='%1.0f%%')\n",
    "    labels=df_attack[col].sum().index\n",
    "    plt.legend(labels=labels, loc=\"upper left\", prop={'size': 7}, bbox_to_anchor=(1,1))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normal records have higher total number of packets transmitted from source to destination and conversely.\n",
    "- Normal records have higher total average of the flow packet size transmitted by the src  and dst.\n",
    "- Normal records have higher total destination packets retransmitted or dropped and for source both Normal records and Exploits Attack are high.\n",
    "- Total number of bit seconds is high in Generic attack records for source and in destination, Normal records are high.\n",
    "- Total number of bytes transaction from src to dst is high in Exploits Attack records and from dst to src is high in Normal records.\n",
    "- Total value of time to live from src to dst is high in Generic Attack and Normal records, but from dst to src is high for Normal records.\n",
    "- Normal records have higher total duration.\n",
    "- Generic Attacks have higher total rate of packets per second in transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Piechart to see distribution of source and destination in each attack_cat:\n",
    "\n",
    "cols=['spkts','sload','sbytes','dpkts','dload','dbytes','sttl','dttl','sloss','dloss','smean','dmean','swin', 'dwin', 'dur', 'rate']\n",
    "df_label = df.groupby('label')\n",
    "plt.figure(figsize=(40,50))\n",
    "\n",
    "for i,col in enumerate(cols):\n",
    "    plt.subplot(7,3,i+1)\n",
    "    df_label[col].sum().plot(kind='bar',  title=(f'Ditribution of {col} in each attack_cat'))\n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Normal label is higher in spkts, dpkts, dloss, dload, dbytes, dttl, dmean.\n",
    "- The Attack lable is higher in sload, sbytes, sttl, sloss, dur, rate.\n",
    "- The Normal and Attack labe have almost the same value for smean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Bivariate analysis of a continuous-categorical pair:\n",
    "plt.figure(figsize=(30,50))\n",
    "for i,col in enumerate(numeric_columns.drop('id')):\n",
    "    plt.subplot(12,4,i+1)\n",
    "    sns.barplot(df['proto'], df[col])\n",
    "    plt.title(f'Distribution of proto by {col}')\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "plt.subplots_adjust(hspace = 1.2, top = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Protocol:\n",
    "\n",
    "- OSPF protocol has most total duration of records.\n",
    "- OSPF protocol has maximum number of packets and SCTP protocol has maximum number of bytes from source to destination.\n",
    "- TCP protocol has maximum number of packets (bytes) from destination to sourc.\n",
    "- OSPF protocol has lowest total transaction packets per second and longest is any.\n",
    "- TCp protocol has lowest time to live value from Source to destination.\n",
    "- UDP protocol has lowest time to live value from destination to source.\n",
    "- OSPF protocol has lowest source bits per second and SCTP protocol has the maximum.\n",
    "- TCP protocol has maximum destination bits per secondbut.\n",
    "- TCP  protocol has maximum number of source and destination packets retransmitted or dropped. \n",
    "- SCTP protocol has maximum mean of the flow packet size transmitted by the src, and for destination TCP protocol is the maximum.\n",
    "- TCP protocol has maximum number of Normal records.\n",
    "- UDP protocol has maximum number of Attack records and SCTP protocol has minimum number of Attack records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution of protocol and label:\n",
    "\n",
    "df.groupby('proto').label.value_counts().plot(kind='barh', title='Distribution of protocol and label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bivariate analysis of a continuous-categorical pair:\n",
    "plt.figure(figsize=(30,50))\n",
    "for i,col in enumerate(numeric_columns.drop('id')):\n",
    "    plt.subplot(12,4,i+1)\n",
    "    sns.barplot(df['state'], df[col])\n",
    "    plt.title(f'Distribution of state by {col}')\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "plt.subplots_adjust(hspace = 1, top = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Transaction State:\n",
    "\n",
    "- REQ state has most total duration of records.\n",
    "- FIN state has maximum number of packets and CON state has maximum number of bytes from source to destination.\n",
    "- FIN state has maximum number of packets (bytes) from destination to sourc.\n",
    "- INT state has Maximum total transaction packets per second and longest is any.\n",
    "- CON state has lowest time to live value from Source to destination.\n",
    "- CLO state has longest time to live value from destination to source and INT , REQ lowest.\n",
    "- INT state has maximum source bits per second.\n",
    "- FIN state has maximum destination bits per secondbut.\n",
    "- CON and FIN  states have maximum number of source packets retransmitted or dropped and FIN for destination. \n",
    "- SCTP state has maximum mean of the flow packet size transmitted by the src, and for destination TCP protocol is the maximum.\n",
    "- FIN state has maximum number of Normal recordsbut less than Attack records.\n",
    "- INT state has maximum number of Attack records and CON, REQ, RST and ACC states have minimum number of Attack records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution of transaction state and label:\n",
    "df.groupby('state').label.value_counts().plot(kind='bar', color='pink', title='Distribution of transaction state and label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis of a continuous-categorical pair:\n",
    "plt.figure(figsize=(30,50))\n",
    "for i,col in enumerate(numeric_columns.drop('id')):\n",
    "    plt.subplot(12,4,i+1)\n",
    "    sns.barplot(df['service'], df[col])\n",
    "    plt.title(f'Distribution of service by {col}')\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "plt.subplots_adjust(hspace = 1, top = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Service:\n",
    "\n",
    "- SSL service has most total duration of records.\n",
    "- SMTP service has maximum number of packets (bytes) from source to destination.\n",
    "- POP3 service has maximum number of packets (bytes) from destination to source.\n",
    "- SMTP service has maximum total transaction packets per second.\n",
    "- SNMP and Radius services have longest time to live value from Source to destination and SSH service is the lowest.\n",
    "- POP3, SSL and IRC services have longest time to live value from destination to source and SNMP is the lowest.\n",
    "- DHCP service has maximum source bits per second.\n",
    "- FTP_data service has maximum destination bits per secondbut.\n",
    "- SMTP service has maximum number of source packets retransmitted or dropped. \n",
    "- POP3 service has maximum number of destination packets retransmitted or dropped.\n",
    "- SMTP service has maximum mean of the flow packet size transmitted by the src, and for destination POP3 service is the maximum.\n",
    "- DNS service has maximum number of Attack records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution of service and label:\n",
    "df.groupby('service').label.value_counts().plot(kind='bar', color='skyblue', title='Distribution of service and label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the distribution of some source and destination columns:\n",
    "cols=['spkts','sload','sbytes','sttl','dpkts','dload','dbytes','dttl','sloss','dloss','smean','dmean','swin', 'dwin', 'dur','rate']\n",
    "\n",
    "plt.figure(figsize=(30,50))\n",
    "for i,col in enumerate(cols):\n",
    "    plt.subplot(5,4,i+1)\n",
    "    sns.distplot(df[col])\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "plt.subplots_adjust(hspace = 0.5, top = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the distribution of source and destination almost the same, in sload,smean abit higher than dload,dmean and dttl higher than sttl. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using violinplot to distribution of rate and attack_cat by label: \n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "\n",
    "sns.catplot(x=\"attack_cat\", y='rate', hue=\"label\", kind=\"violin\", split=False, data=df)\n",
    "plt.title('Distribution of attack_cat and rate by label')\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using violinplot to distribution of duration and attack_cat by label: \n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "\n",
    "sns.catplot(x=\"attack_cat\", y=\"dur\", hue=\"label\", kind=\"violin\", split=False, data=df)\n",
    "plt.title('Distribution of attack_cat and duration by label')\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using boxen plot to distribution of rate and attack_cat by state:\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "\n",
    "sns.catplot(x=\"attack_cat\", y=\"rate\", hue=\"state\", kind=\"boxen\", data=df)\n",
    "plt.title('Distribution of attack_cat and rate by state')\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using boxen plot to distribution of duration and attack_cat by state:\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "\n",
    "sns.catplot(x=\"attack_cat\", y=\"dur\", hue=\"state\", kind=\"boxen\", data=df)\n",
    "plt.title('Distribution of attack_cat and duration by state')\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using barplot to distribution of rate and attack_cat by protocol:\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "\n",
    "sns.catplot(x=\"attack_cat\", y=\"rate\", hue=\"proto\", kind=\"bar\", data=df)\n",
    "plt.title('Distribution of attack_cat and rate by protocol')\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using barplot to distribution of duration and attack_cat by protocol:\n",
    "\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "\n",
    "sns.catplot(x=\"attack_cat\", y=\"dur\", hue=\"proto\", kind=\"bar\", data=df)\n",
    "plt.title('Distribution of attack_cat and duration by protocol')\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using barplot to distribution of rate and attack_cat by service:\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "\n",
    "sns.catplot(x=\"attack_cat\", y=\"rate\", hue=\"service\", kind=\"bar\", data=df)\n",
    "plt.title('Distribution of attack_cat and rate by service')\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using barplot to distribution of duration and attack_cat by service:\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "\n",
    "sns.catplot(x=\"attack_cat\", y=\"dur\", hue=\"service\", kind=\"bar\", data=df)\n",
    "plt.title('Distribution of attack_cat and duration by service')\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using boxplot to display range of average packets size transmittd by source in each category attack and label:\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.boxplot(x='attack_cat',y='smean',hue='label',data=df)  \n",
    "plt.title('Distribution of attack_cat and smean by label')\n",
    "\n",
    "sns.despine(offset=10, trim=True)\n",
    "ax.set(xlabel='attack_cat', ylabel='smean')\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using boxplot to display range of average packets size transmittd by destination in each category attack and label:\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.boxplot(x='attack_cat',y='dmean',hue='label',data=df)  \n",
    "plt.title('Distribution of Attack_cat and dmean by label')\n",
    "\n",
    "sns.despine(offset=10, trim=True)\n",
    "ax.set(xlabel='attack_cat', ylabel='dmean')\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using boxplot to display Source TCP window advertisement value in each category attack and label:\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.boxplot(x='attack_cat',y='swin',hue='label',data=df)  \n",
    "plt.title('Distribution of Attack_cat and swin by label')\n",
    "\n",
    "sns.despine(offset=10, trim=True)\n",
    "ax.set(xlabel='attack_cat', ylabel='swin')\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using boxplot to display destination TCP window advertisement value in each category attack and label:\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.boxplot(x='attack_cat',y='dwin',hue='label',data=df)  \n",
    "plt.title('Distribution of Attack_cat and dwin by label')\n",
    "\n",
    "sns.despine(offset=10, trim=True)\n",
    "ax.set(xlabel='attack_cat', ylabel='dwin')\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,5))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "sns.boxplot(x='attack_cat', y='dur', data=df)\n",
    "plt.title('Duration of attack_cat')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "sns.boxplot(x='attack_cat', y='rate', data=df)\n",
    "plt.title('Rating of attack_cat')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplots_adjust(hspace = 1.2, top = 0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of attack_cat with service, state, protocol by rate: \n",
    "plt.figure(figsize=(25,10))\n",
    "\n",
    "\n",
    "df.groupby(['attack_cat', 'service']).rate.mean().plot(kind = 'line', color = 'green', label = 'service', linewidth=1, alpha = 0.5, grid = True)\n",
    "\n",
    "df.groupby(['attack_cat', 'state']).rate.mean().plot(kind = 'line', color = 'blue', label = 'state', linewidth=1, alpha = 0.5, grid = True)\n",
    "\n",
    "df.groupby(['attack_cat', 'proto']).rate.mean().plot(kind = 'line', color = 'purple', label = 'protocol', linewidth=1, alpha = 0.5, grid = True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribution of attack_cat with service, state, protocol by duration: \n",
    "plt.figure(figsize=(25,10))\n",
    "\n",
    "df.groupby(['attack_cat', 'service']).dur.mean().plot(kind = 'line', color = 'blue', label = 'service', linewidth=1, alpha = 0.5, grid = True)\n",
    "\n",
    "df.groupby(['attack_cat', 'state']).dur.mean().plot(kind = 'line', color = 'purple', label = 'state', linewidth=1, alpha = 0.5, grid = True)\n",
    "\n",
    "df.groupby(['attack_cat', 'proto']).dur.mean().plot(kind = 'line' , color = 'green', label = 'protocol', linewidth=1, alpha = 0.5, grid = True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of service , state, protocol with label by rate: \n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "df.groupby(['service', 'label']).rate.mean().plot(kind = 'bar', color = 'skyblue', label = 'service', linewidth=1, alpha = 0.5, grid = True)\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "df.groupby(['state', 'label']).rate.mean().plot(kind = 'bar', color = 'purple', label = 'state', linewidth=1, alpha = 0.5, grid = True)\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "df.groupby(['proto', 'label']).rate.mean().plot(kind = 'bar', color = 'pink', label = 'protocol', linewidth=1, alpha = 0.5, grid = True)\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.subplots_adjust(hspace = 1.2, top = 0.9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of service , state, protocol with label by duration: \n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "df.groupby(['service', 'label']).dur.mean().plot(kind = 'bar', color = 'skyblue', label = 'service', linewidth=1, alpha = 0.5, grid = True)\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "df.groupby(['state', 'label']).dur.mean().plot(kind = 'bar', color = 'purple', label = 'state', linewidth=1, alpha = 0.5, grid = True)\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "df.groupby(['proto', 'label']).dur.mean().plot(kind = 'bar', color = 'pink', label = 'protocol', linewidth=1, alpha = 0.5, grid = True)\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "plt.subplots_adjust(hspace = 1.2, top = 0.9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the distribution of target variable because target variable is binary use boxplot instead of hist plot:\n",
    "plt.figure(figsize=(15,5))###???? im not sure choose label or attack_cat as a target??????\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(df['label'])\n",
    "plt.title('Distribution of label Attack')\n",
    "plt.xlabel(\"Attack\")\n",
    "plt.ylabel(\"Number of Occurrence\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(df['label'])\n",
    "plt.title('Distribution of label Attack')\n",
    "plt.xlabel(\"Attack\")\n",
    "plt.ylabel(\"Number of Occurrence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution of target variable.#####???? can we choose both(label, attack_cat) as target variable??????\n",
    "\n",
    "sns.countplot(df['attack_cat'])         ###? can use hist for object type???type of plot is correct??????\n",
    "plt.title('Distribution of categories Attack')\n",
    "plt.xlabel(\"Categories of Attack\")\n",
    "plt.ylabel(\"Number of Occurrence\")\n",
    "plt.xticks(rotation =90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using T_test to determine if there is a significant difference between the Normal and Attack records in rate:\n",
    "\n",
    "# Use reset_index because i want to change panda series to df nead to be old index:\n",
    "\n",
    "normal_record=df[df.label== 0].groupby('attack_cat').rate.sum()\n",
    "normal_record=np.array(normal_record)\n",
    "\n",
    "attack_record= df[df.label==1].groupby('attack_cat').rate.sum()\n",
    "attack_record=np.array(attack_record)\n",
    "\n",
    "scipy.stats.ttest_ind(normal_record, attack_record, equal_var=False)\n",
    "\n",
    "####?????for ttest choose correct column?????why get nan?????what does t test is parametric test means???\n",
    "###????what is reset_index use for????????????????\n",
    "### ???? what does mannwhitneyu means and use for what?????/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing data for modeling:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For modeling,need all columns to be numeric. To convert nonnumeric to numeric values, I can either use dummy variables or encode them. By using dummy, we can make  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nonnumeric column to numeric by using encoding:\n",
    "                     #####?????what is different between cat.codes and label encoding???????\n",
    "categorical = df.select_dtypes(include=['object']).drop('attack_cat', axis=1)\n",
    "dummies = pd.get_dummies(categorical, drop_first=True)\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nonnumeric columns variables after converting to dummies: \n",
    "df = df.drop(list(categorical.columns), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dummies variables with dataset:\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Findout label assigned to attack_cat:\n",
    "\n",
    "#c = df_main['attack_cat'].astype('category')\n",
    "#dic = dict(enumerate(c.cat.categories))\n",
    "#df['code'] = df_main.attack_cat.astype('category').cat.codes\n",
    "#df['attack_name'] = df['code'].map(dic)\n",
    "\n",
    "\n",
    "#dummies_attack_cat=[col for col in df if col.startswith('attack_cat')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to create the necessary training and test groups:\n",
    "x = df.drop(['attack_cat', 'id'], axis=1)\n",
    "y = df['attack_cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying Models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "    \n",
    "    train_probs = model.predict_proba(X_train)\n",
    "    test_probs = model.predict_proba(X_test)\n",
    "    \n",
    "    print('{} has training accuracy of: {}'.format(model_name, accuracy_score(y_train, train_preds)))\n",
    "    print('{} has test accuracy of: {}\\n'.format(model_name, accuracy_score(y_test, test_preds)))\n",
    "    \n",
    "    print('{} has training log loss of: {}'.format(model_name, log_loss(y_train, train_probs)))\n",
    "    print('{} has test log loss of: {}\\n'.format(model_name, log_loss(y_test, test_probs)))\n",
    "    \n",
    "    return train_preds, test_preds, train_probs, test_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1- Preliminary Logistic Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Applying logistic Regression model:\n",
    "\n",
    "lr_initial = LogisticRegression( n_jobs=-1)\n",
    "\n",
    "initial_lr_train_preds, initial_lr_test_preds, initial_lr_train_probs, initial_lr_test_probs = get_scores(lr_initial, ' Preliminary Logistic Regression model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2- Preliminary K Neighbors Classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying KNeighbors Classifier model:\n",
    "\n",
    "\n",
    "knn_initial = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "initial_knn_train_preds, initial_knn_test_preds, initial_knn_train_probs, initial_knn_test_probs = get_scores(knn_initial, 'Preliminary knn model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3- Preliminary Random Forest Classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Applying Random Forest Classifier model:\n",
    "\n",
    "rfc_initial = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "initial_rfc_train_preds, initial_rfc_test_preds, initial_rfc_train_probs, initial_rfc_test_probs = get_scores(rfc_initial,'Preliminary Random Forest model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4- Preliminary Support Vector Classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Support Vector Classifier: why took long time ???????\n",
    "\n",
    "#svc_initial = SVC(gamma='auto', probability=True)\n",
    "\n",
    "#initial_svc_train_preds, initial_svc_test_preds, initial_svc_train_probs, initial_svc_test_probs = get_scores(svc_initial,'Preliminary Support Vector Classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5- Preliminary Gradiant Boosting Classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Gradoant Boosting Classifier:\n",
    "\n",
    "gbc_initial = GradientBoostingClassifier()\n",
    "\n",
    "initial_gbc_train_preds, initial_gbc_test_preds, initial_gbc_train_probs, initial_gbc_test_probs = get_scores(gbc_initial, 'Preliminary Gradient Boosting model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataFrame with accuracy of different models using dictionary:\n",
    "preliminary_model_accuracy=pd.DataFrame({\"Models\":['Initial Logistic Regression', 'Initial knn', 'Initial Random Forest', 'Initial Gradient Boosting'], \n",
    "                 \"Training Accuracy\":[0.63,0.77,0.94,0.9],\n",
    "                 \"Test Accuracy\":[0.63,0.69,0.89, 0.9],\n",
    "                 \"Training Log Loss\":[1.38,1.04,0.16,0.31],\n",
    "                 \"Test Log Loss\":[1.37,4.6,0.74,0.32]}) \n",
    " \n",
    "preliminary_model_accuracy###>??? is there any way to get value??????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in above summary, training accuracies range (Number of correct predictions /Total number of predictions) from 63% to 94% and test accuracies range between 63% to 90%. The knn model has the most overfitting because the value of test accuracy is much lower than training accuracy compare with other models. These overfitting trends are similar in the log loss scoring(log loss:uncertainly). So, the best test accuracy and test log loss score goes to Gradient Boosting model. should be noted that these initial models are not optimized; I only use the default hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy by Attack:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Findout models predict a certain type of attack particularly well or terribly:\n",
    "\n",
    "def get_accuracies(predict, y_true):\n",
    "    y_true = y_true.reset_index()###????? im not sure is it correct????\n",
    "    accuracy_lst = []\n",
    "    for attack in df['attack_cat'].unique(): \n",
    "        count = 0\n",
    "        for i in y_true[y_true==attack].index:\n",
    "            if predict[i] == attack:\n",
    "                count += 1\n",
    "        accuracy_lst.append(count/y_true[y_true==attack].shape[0]*100)\n",
    "    return accuracy_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Findout the accuracy of each model for attack type:\n",
    "\n",
    "lr_train_accuracies = get_accuracies(initial_lr_train_preds, y_train)\n",
    "lr_test_accuracies = get_accuracies(initial_lr_test_preds, y_test)\n",
    "\n",
    "knn_train_accuracies = get_accuracies(initial_knn_train_preds, y_train)\n",
    "knn_test_accuracies = get_accuracies(initial_knn_test_preds, y_test) \n",
    "\n",
    "rfc_train_accuracies = get_accuracies(initial_rfc_train_preds, y_train)\n",
    "rfc_test_accuracies = get_accuracies(initial_rfc_test_preds, y_test) \n",
    "\n",
    "#svc_train_accuracies = get_accuracies(initial_svc_train_preds, y_train)\n",
    "#svc_test_accuracies = get_accuracies(initial_svc_test_preds, y_test)\n",
    "\n",
    "gbc_train_accuracies = get_accuracies(initial_gbc_train_preds, y_train)\n",
    "gbc_test_accuracies = get_accuracies(initial_gbc_test_preds, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the test accuracies for each model and attack type by using heatmap:\n",
    "\n",
    "models_accuracies = {'lr': lr_test_accuracies, 'knn': knn_test_accuracies, 'rfc': rfc_test_accuracies, 'gbc': gbc_test_accuracies}\n",
    "initial_df_test_accuracy = pd.DataFrame(models_accuracies, index = sorted(df['attack_cat'].unique()), columns = ['lr', 'knn', 'rfc', 'gbc'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "\n",
    "sns.heatmap(initial_df_test_accuracy.T, cmap = 'coolwarm', square = True, linewidths=0.1, annot=True)\n",
    "plt.title('preliminary Test Accuracies', fontsize = 16)\n",
    "plt.xlabel('Attack', fontsize = 13)\n",
    "plt.ylabel('Model', fontsize = 13)\n",
    "plt.tick_params(axis='both', which='major', labelsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in heatmap plot to look at the accuracies for each model and attack type to predict, the highest accuracy came from Logistic Regression model with an accuracy of 61% for Analysis. Overall, Analysis Attack has highest accuracy in all model and models have very low accuracy to predict other attack types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_ Improving Scores**\n",
    "\n",
    "- Feature Engineering: \n",
    "\n",
    "  I've already done a bit of feature engineering by converting nonnumeric columns to numeric. \n",
    "\n",
    "   - Using PCA for dimentional reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA for feature reduction: \n",
    "X = df.drop(['attack_cat', 'id'], axis = 1)\n",
    "Y = df['attack_cat']      \n",
    "\n",
    "x = StandardScaler().fit_transform(X)\n",
    "pca = PCA(0.90)\n",
    "principalComponents = pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the pca components:\n",
    "print(abs( pca.components_ )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Findout number of components explained 90% of variance in the dataset:\n",
    "pca_number = pca.n_components_\n",
    "print(pca_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the percentage of total variance in the dataset explained by each components:\n",
    "print(\n",
    "    'The percentage of total variance in the dataset explained by each, component from Sklearn PCA.\\n',\n",
    "    pca.explained_variance_ratio_ ,pca.explained_variance_ratio_.sum() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PCA to dataframe:\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['pca' + str(i) for i in range (1, pca_number+1)])\n",
    "principalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat PCA with target variable:\n",
    "principalDf['attack_cat'] = df['attack_cat']\n",
    "principalDf.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = principalDf.drop('attack_cat', 1)\n",
    "y = principalDf['attack_cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Logistic Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying logistic Regression model After applying PCA:\n",
    "\n",
    "lr_initial = LogisticRegression( n_jobs=-1)\n",
    "\n",
    "initial_lr_train_preds, initial_lr_test_preds, initial_lr_train_probs, initial_lr_test_probs = get_scores(lr_initial, ' Preliminary Logistic Regression model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 K Neighbors Classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying KNeighbors Classifier model:\n",
    "\n",
    "\n",
    "knn_initial = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "initial_knn_train_preds, initial_knn_test_preds, initial_knn_train_probs, initial_knn_test_probs = get_scores(knn_initial, 'Preliminary knn model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Random Forest Classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Random Forest Classifier model:\n",
    "\n",
    "rfc_initial = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "initial_rfc_train_preds, initial_rfc_test_preds, initial_rfc_train_probs, initial_rfc_test_probs = get_scores(rfc_initial,'Preliminary Random Forest model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Support Vector Classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Support Vector Classifier: why took long time ???????\n",
    "\n",
    "#svc_initial = SVC(gamma='auto', probability=True)\n",
    "\n",
    "#initial_svc_train_preds, initial_svc_test_preds, initial_svc_train_probs, initial_svc_test_probs = get_scores(svc_initial,'Preliminary Support Vector Classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1 Preliminary Gradiant Boosting Classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Gradoant Boosting Classifier:\n",
    "\n",
    "gbc_initial = GradientBoostingClassifier()\n",
    "\n",
    "initial_gbc_train_preds, initial_gbc_test_preds, initial_gbc_train_probs, initial_gbc_test_probs = get_scores(gbc_initial, 'Preliminary Gradient Boosting model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataFrame with accuracy of different models using dictionary:\n",
    "applying_pca_model_accuracy=pd.DataFrame({\"Models\":['Initial Logistic Regression', 'Initial knn', 'Initial Random Forest', 'Initial Gradient Boosting'], \n",
    "                 \"Training Accuracy PCA\":[0.86,0.89,0.94,0.89],\n",
    "                 \"Test Accuracy PCA\":[0.86,0.87,0.87, 0.87],\n",
    "                 \"Training Log Loss PCA\":[0.4,0.74,0.17,0.31],\n",
    "                 \"Test Log Loss PCA\":[0.4,1.75,1.00,0.36]}) \n",
    " \n",
    "applying_pca_model_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in above summary, training accuracies after applying PCA is in range (Number of correct predictions /Total number of predictions) from 86% to 94% and test accuracies range between 66% to 87%. The Random Forest model has the most overfitting because the value of test accuracy is much lower than training accuracy compare with other models. These overfitting trends are similar in the log loss scoring(log loss:uncertainly). So, the best test accuracy and test log loss score goes to Gradient Boosting model. should be noted that these initial models are not optimized; I only use the default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Findout models predict a certain type of attack particularly well or terribly:\n",
    "\n",
    "def get_accuracies(predict, y_true):\n",
    "    y_true = y_true.reset_index()###????? im not sure is it correct????\n",
    "    accuracy_lst = []\n",
    "    for attack in df['attack_cat'].unique(): \n",
    "        count = 0\n",
    "        for i in y_true[y_true==attack].index:\n",
    "            if predict[i] == attack:\n",
    "                count += 1\n",
    "        accuracy_lst.append(count/y_true[y_true==attack].shape[0]*100)\n",
    "    return accuracy_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Findout the accuracy of each model for attack type:\n",
    "\n",
    "lr_train_accuracies = get_accuracies(initial_lr_train_preds, y_train)\n",
    "lr_test_accuracies = get_accuracies(initial_lr_test_preds, y_test)\n",
    "\n",
    "knn_train_accuracies = get_accuracies(initial_knn_train_preds, y_train)\n",
    "knn_test_accuracies = get_accuracies(initial_knn_test_preds, y_test) \n",
    "\n",
    "rfc_train_accuracies = get_accuracies(initial_rfc_train_preds, y_train)\n",
    "rfc_test_accuracies = get_accuracies(initial_rfc_test_preds, y_test) \n",
    "\n",
    "#svc_train_accuracies = get_accuracies(initial_svc_train_preds, y_train)\n",
    "#svc_test_accuracies = get_accuracies(initial_svc_test_preds, y_test)\n",
    "\n",
    "gbc_train_accuracies = get_accuracies(initial_gbc_train_preds, y_train)\n",
    "gbc_test_accuracies = get_accuracies(initial_gbc_test_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the test accuracies for each model and attack type by using heatmap:\n",
    "\n",
    "models_accuracies = {'lr': lr_test_accuracies, 'knn': knn_test_accuracies, 'rfc': rfc_test_accuracies, 'gbc': gbc_test_accuracies}\n",
    "initial_df_test_accuracy = pd.DataFrame(models_accuracies, index = sorted(df['attack_cat'].unique()), columns = ['lr', 'knn', 'rfc', 'gbc'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "\n",
    "sns.heatmap(initial_df_test_accuracy.T, cmap = 'coolwarm', square = True, linewidths=0.1, annot=True)\n",
    "plt.title('preliminary Test Accuracies', fontsize = 16)\n",
    "plt.xlabel('Attack', fontsize = 13)\n",
    "plt.ylabel('Model', fontsize = 13)\n",
    "plt.tick_params(axis='both', which='major', labelsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after applying PCA,the accuracy of all models get a bit high but not much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding External Sources:\n",
    "\n",
    "I add BoT-IoT  dataset which is new approaches of authors for developing, Intrusion Detection and threat intelligence approaches in different systems, such as Network Systems.\n",
    "\n",
    "The BoT-IoT dataset was created by designing a realistic network environment in the Cyber Range Lab of The center of UNSW Canberra Cyber. The environment incorporates a combination of normal and botnet traffic. The dataset’s source files are provided in csv files. The files were separated, based on attack category and subcategory, to better assist in labeling process. The dataset includes DDoS, DoS, OS and Service Scan, Keylogging and Data exfiltration attacks, with the DDoS and DoS attacks further organized, based on the protocol used. To ease the handling of the dataset, I used top 10 features of  5% of dataset which is configured as a training set. namely: UNSW_2018_IoT_Botnet_Final_10_best_Training.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new dataset:\n",
    "df_new = pd.read_csv(r'C:\\Users\\mebra.DESKTOP-L12LJA6\\Thinkful Works\\PythonThinkful\\capstonbotdataset\\UNSW_2018_IoT_Botnet_Final_10_best_Training.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the new dataset:\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the shape of new dataset:\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the type of new dataset:\n",
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Findout object columns:\n",
    "\n",
    "df_new_object_columns = df_new.select_dtypes('object')\n",
    "df_new_object_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.rename(columns={\"Attack\" : \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_main['proto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b2f2e7e129a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Merge the dataset with new one:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_main\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_main\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'proto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   6866\u001b[0m                      \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6867\u001b[0m                      \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6868\u001b[1;33m                      copy=copy, indicator=indicator, validate=validate)\n\u001b[0m\u001b[0;32m   6869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6870\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     45\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                          validate=validate)\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_specification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[1;31m# note this function has side effects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_validate_specification\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1055\u001b[0m                                      'of levels in the index of \"left\"')\n\u001b[0;32m   1056\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_on\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1057\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_on\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_on\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1058\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"len(right_on) must equal len(left_on)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Merge the dataset with new one:\n",
    "\n",
    "df_main = df_main.merge(df_new, on ='proto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the new dataset:\n",
    "df_final.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
