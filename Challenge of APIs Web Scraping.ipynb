{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge**\n",
    "\n",
    "Do a little scraping or API-calling of your own. Pick a new website and see what you can get out of it. Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.\n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1. Return specific pieces of information (rather than just downloading a whole page)\n",
    "2. Iterate over multiple pages/queries\n",
    "3. Save the data to your computer\n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest. Write up a report from scraping code to summary and share it with your mentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 links extracted!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class CovidWikiSpider(scrapy.Spider):\n",
    "    name = \"CWS\"\n",
    "    \n",
    "    start_urls = [\n",
    "        'https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Coronavirus_disease_2019&lhprop=title%7Credirect'\n",
    "        ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//lh'):\n",
    "            if item.xpath('@ns').extract_first() == '0':\n",
    "                yield {\n",
    "                    'title': item.xpath('@title').extract_first()\n",
    "                    \n",
    "                    }\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'covid.json',\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "process.crawl(CovidWikiSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95, 1)\n",
      "                             title\n",
      "0     Traditional Chinese medicine\n",
      "1                           Corona\n",
      "2      List of infectious diseases\n",
      "3                       Jim Bakker\n",
      "4  National Basketball Association\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "covid=pd.read_json('covid.json', orient='records')\n",
    "print(covid.shape)\n",
    "print(covid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
